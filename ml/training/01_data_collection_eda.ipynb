{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä TourismIQ - Notebook 1: Data Collection & EDA\n",
    "\n",
    "**Objectif**: Explorer les donn√©es collect√©es et pr√©parer le terrain pour le Quality Scorer\n",
    "\n",
    "## Donn√©es disponibles\n",
    "- **DATAtourisme**: 50k POIs (labels, descriptions, g√©oloc, types)\n",
    "- **Opendatasoft**: 10k communes (population, superficie, densit√©)\n",
    "- **Open-Meteo**: 13 r√©gions (climat, temp√©rature, pr√©cipitations)\n",
    "\n",
    "## Questions cl√©s\n",
    "1. Quelle est la structure des POIs DATAtourisme ?\n",
    "2. Quel est le taux de compl√©tude des champs importants ?\n",
    "3. Quelle est la distribution g√©ographique des POIs ?\n",
    "4. Quels types de POIs avons-nous ?\n",
    "5. Quelles features utiliser pour le Quality Scorer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config viz\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des fichiers\n",
    "DATA_DIR = Path(\"../data/raw\")\n",
    "\n",
    "pois_file = DATA_DIR / \"datatourisme_pois_50k.parquet\"\n",
    "communes_file = DATA_DIR / \"communes_population_all.parquet\"\n",
    "climate_file = DATA_DIR / \"climate_regions.parquet\"\n",
    "\n",
    "# V√©rifier existence\n",
    "print(\"üìÅ Fichiers disponibles:\")\n",
    "print(f\"  POIs: {pois_file.exists()} ({pois_file.stat().st_size / 1024 / 1024:.2f} MB)\" if pois_file.exists() else \"  POIs: ‚ùå\")\n",
    "print(f\"  Communes: {communes_file.exists()} ({communes_file.stat().st_size / 1024:.2f} KB)\" if communes_file.exists() else \"  Communes: ‚ùå\")\n",
    "print(f\"  Climat: {climate_file.exists()} ({climate_file.stat().st_size / 1024:.2f} KB)\" if climate_file.exists() else \"  Climat: ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger POIs\n",
    "print(\"üìÇ Chargement des POIs...\")\n",
    "df_pois = pd.read_parquet(pois_file)\n",
    "print(f\"‚úÖ {len(df_pois):,} POIs charg√©s\")\n",
    "print(f\"   Colonnes: {len(df_pois.columns)}\")\n",
    "print(f\"   M√©moire: {df_pois.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger communes\n",
    "print(\"üìÇ Chargement des communes...\")\n",
    "df_communes = pd.read_parquet(communes_file)\n",
    "print(f\"‚úÖ {len(df_communes):,} communes charg√©es\")\n",
    "print(f\"   Colonnes: {list(df_communes.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger climat\n",
    "print(\"üìÇ Chargement du climat...\")\n",
    "df_climate = pd.read_parquet(climate_file)\n",
    "print(f\"‚úÖ {len(df_climate)} r√©gions charg√©es\")\n",
    "print(f\"   Colonnes: {list(df_climate.columns)}\")\n",
    "df_climate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Structure des POIs DATAtourisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu g√©n√©ral\n",
    "print(\"üìä Informations g√©n√©rales:\")\n",
    "print(f\"  Shape: {df_pois.shape}\")\n",
    "print(f\"  Colonnes: {len(df_pois.columns)}\")\n",
    "print(f\"\\nüìã Liste des colonnes:\")\n",
    "for i, col in enumerate(df_pois.columns, 1):\n",
    "    dtype = df_pois[col].dtype\n",
    "    non_null = df_pois[col].notna().sum()\n",
    "    pct = non_null / len(df_pois) * 100\n",
    "    print(f\"  {i:2d}. {col:30s} - {str(dtype):10s} - {non_null:6,} ({pct:5.1f}%) non-null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âchantillon de donn√©es\n",
    "print(\"üîç √âchantillon (premier POI):\")\n",
    "df_pois.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse de Compl√©tude\n",
    "\n",
    "Pour le Quality Scorer, on doit √©valuer la compl√©tude des champs importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champs cl√©s pour le Quality Score\n",
    "key_fields = [\n",
    "    '@id',           # ID unique\n",
    "    '@type',         # Type de POI\n",
    "    'rdfs:label',    # Nom/label\n",
    "    'hasDescription',# Description\n",
    "    'isLocatedAt',   # Localisation\n",
    "    'hasContact',    # Contact\n",
    "    'hasBeenCreatedBy', # Cr√©ateur/source\n",
    "    'hasBeenPublishedBy', # √âditeur\n",
    "]\n",
    "\n",
    "print(\"üìä Compl√©tude des champs cl√©s:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "completeness = {}\n",
    "for field in key_fields:\n",
    "    if field in df_pois.columns:\n",
    "        non_null = df_pois[field].notna().sum()\n",
    "        pct = non_null / len(df_pois) * 100\n",
    "        completeness[field] = pct\n",
    "        print(f\"{field:25s}: {non_null:6,} / {len(df_pois):6,} ({pct:5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{field:25s}: ABSENT\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation compl√©tude\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fields = list(completeness.keys())\n",
    "values = list(completeness.values())\n",
    "\n",
    "colors = ['green' if v > 80 else 'orange' if v > 50 else 'red' for v in values]\n",
    "bars = ax.barh(fields, values, color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Compl√©tude (%)', fontsize=12)\n",
    "ax.set_title('Compl√©tude des Champs Cl√©s - DATAtourisme POIs', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.axvline(x=80, color='green', linestyle='--', alpha=0.3, label='Excellent (>80%)')\n",
    "ax.axvline(x=50, color='orange', linestyle='--', alpha=0.3, label='Moyen (>50%)')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Ajouter valeurs\n",
    "for i, (field, value) in enumerate(zip(fields, values)):\n",
    "    ax.text(value + 2, i, f'{value:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction et Parsing des Champs Complexes\n",
    "\n",
    "Les champs DATAtourisme sont souvent en JSON (nested). On doit les parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: parser JSON strings\n",
    "def parse_json_field(value):\n",
    "    \"\"\"Parse un champ qui peut √™tre string JSON ou d√©j√† pars√©\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except:\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "print(\"‚úÖ Helper parse_json_field d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple: parser @type (type de POI)\n",
    "print(\"üîç Analyse du champ @type:\")\n",
    "print(\"\\nExemples de valeurs:\")\n",
    "for i in range(min(5, len(df_pois))):\n",
    "    type_val = df_pois['@type'].iloc[i] if '@type' in df_pois.columns else None\n",
    "    print(f\"  {i+1}. {type_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire types principaux\n",
    "def extract_main_type(type_val):\n",
    "    \"\"\"Extrait le type principal d'un POI\"\"\"\n",
    "    type_val = parse_json_field(type_val)\n",
    "    if isinstance(type_val, list):\n",
    "        # Prendre le type le plus sp√©cifique (souvent le premier non-g√©n√©rique)\n",
    "        for t in type_val:\n",
    "            if t not in ['schema:Thing', 'schema:Place', 'olo:OrderedList']:\n",
    "                return t\n",
    "        return type_val[0] if type_val else None\n",
    "    return type_val\n",
    "\n",
    "if '@type' in df_pois.columns:\n",
    "    df_pois['type_principal'] = df_pois['@type'].apply(extract_main_type)\n",
    "    print(\"‚úÖ Colonne 'type_principal' cr√©√©e\")\n",
    "    print(f\"\\nüìä Distribution des types (top 20):\")\n",
    "    print(df_pois['type_principal'].value_counts().head(20))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Colonne @type absente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser distribution des types\n",
    "if 'type_principal' in df_pois.columns:\n",
    "    top_types = df_pois['type_principal'].value_counts().head(15)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    top_types.plot(kind='barh', ax=ax, color='steelblue', alpha=0.7)\n",
    "    ax.set_xlabel('Nombre de POIs', fontsize=12)\n",
    "    ax.set_title('Top 15 Types de POIs - DATAtourisme', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Ajouter valeurs\n",
    "    for i, v in enumerate(top_types.values):\n",
    "        ax.text(v + 50, i, f'{v:,}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction GPS & Distribution G√©ographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire coordonn√©es GPS\n",
    "def extract_coordinates(located_at):\n",
    "    \"\"\"Extrait latitude, longitude depuis isLocatedAt\"\"\"\n",
    "    located_at = parse_json_field(located_at)\n",
    "    if not located_at or not isinstance(located_at, list):\n",
    "        return None, None\n",
    "    \n",
    "    for location in located_at:\n",
    "        if isinstance(location, dict) and 'schema:geo' in location:\n",
    "            geo = location['schema:geo']\n",
    "            if isinstance(geo, dict):\n",
    "                lat = geo.get('schema:latitude')\n",
    "                lon = geo.get('schema:longitude')\n",
    "                if lat and lon:\n",
    "                    try:\n",
    "                        return float(lat), float(lon)\n",
    "                    except:\n",
    "                        pass\n",
    "    return None, None\n",
    "\n",
    "print(\"üó∫Ô∏è  Extraction des coordonn√©es GPS...\")\n",
    "if 'isLocatedAt' in df_pois.columns:\n",
    "    coords = df_pois['isLocatedAt'].apply(extract_coordinates)\n",
    "    df_pois['latitude'] = coords.apply(lambda x: x[0])\n",
    "    df_pois['longitude'] = coords.apply(lambda x: x[1])\n",
    "    \n",
    "    # Stats\n",
    "    pois_with_coords = df_pois['latitude'].notna().sum()\n",
    "    print(f\"‚úÖ POIs avec coordonn√©es: {pois_with_coords:,} / {len(df_pois):,} ({pois_with_coords/len(df_pois)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Colonne isLocatedAt absente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution g√©ographique\n",
    "if 'latitude' in df_pois.columns and df_pois['latitude'].notna().sum() > 0:\n",
    "    df_geo = df_pois[df_pois['latitude'].notna()].copy()\n",
    "    \n",
    "    print(f\"üìç Distribution g√©ographique ({len(df_geo):,} POIs):\")\n",
    "    print(f\"  Latitude: {df_geo['latitude'].min():.4f} √† {df_geo['latitude'].max():.4f}\")\n",
    "    print(f\"  Longitude: {df_geo['longitude'].min():.4f} √† {df_geo['longitude'].max():.4f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Densit√© avec hexbin\n",
    "    hb = ax.hexbin(df_geo['longitude'], df_geo['latitude'], \n",
    "                   gridsize=50, cmap='YlOrRd', alpha=0.7, mincnt=1)\n",
    "    \n",
    "    ax.set_xlabel('Longitude', fontsize=12)\n",
    "    ax.set_ylabel('Latitude', fontsize=12)\n",
    "    ax.set_title('Distribution G√©ographique des POIs en France', fontsize=14, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Colorbar\n",
    "    cb = plt.colorbar(hb, ax=ax)\n",
    "    cb.set_label('Densit√© de POIs', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Pas de coordonn√©es GPS disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extraction Descriptions & Analyse Textuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire description FR\n",
    "def extract_description_fr(desc_list):\n",
    "    \"\"\"Extrait description courte en fran√ßais\"\"\"\n",
    "    desc_list = parse_json_field(desc_list)\n",
    "    if not desc_list or not isinstance(desc_list, list):\n",
    "        return None\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        if isinstance(desc, dict) and 'shortDescription' in desc:\n",
    "            short_desc = desc['shortDescription']\n",
    "            if isinstance(short_desc, dict):\n",
    "                # Essayer plusieurs cl√©s pour le fran√ßais\n",
    "                return short_desc.get('fr') or short_desc.get('@fr') or short_desc.get('fr-FR')\n",
    "            elif isinstance(short_desc, str):\n",
    "                return short_desc\n",
    "    return None\n",
    "\n",
    "print(\"üìù Extraction des descriptions...\")\n",
    "if 'hasDescription' in df_pois.columns:\n",
    "    df_pois['description'] = df_pois['hasDescription'].apply(extract_description_fr)\n",
    "    \n",
    "    # Stats\n",
    "    pois_with_desc = df_pois['description'].notna().sum()\n",
    "    print(f\"‚úÖ POIs avec description: {pois_with_desc:,} / {len(df_pois):,} ({pois_with_desc/len(df_pois)*100:.1f}%)\")\n",
    "    \n",
    "    # Longueur descriptions\n",
    "    df_pois['description_length'] = df_pois['description'].fillna('').str.len()\n",
    "    \n",
    "    print(f\"\\nüìä Statistiques longueur descriptions:\")\n",
    "    print(df_pois['description_length'].describe())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Colonne hasDescription absente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution longueur descriptions\n",
    "if 'description_length' in df_pois.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogramme\n",
    "    desc_lengths = df_pois[df_pois['description_length'] > 0]['description_length']\n",
    "    axes[0].hist(desc_lengths, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Longueur (caract√®res)', fontsize=11)\n",
    "    axes[0].set_ylabel('Nombre de POIs', fontsize=11)\n",
    "    axes[0].set_title('Distribution Longueur Descriptions', fontsize=12, fontweight='bold')\n",
    "    axes[0].axvline(desc_lengths.median(), color='red', linestyle='--', label=f'M√©diane: {desc_lengths.median():.0f}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Boxplot\n",
    "    axes[1].boxplot(desc_lengths, vert=True)\n",
    "    axes[1].set_ylabel('Longueur (caract√®res)', fontsize=11)\n",
    "    axes[1].set_title('Boxplot Longueur Descriptions', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de descriptions\n",
    "if 'description' in df_pois.columns:\n",
    "    print(\"üìù Exemples de descriptions:\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_indices = df_pois[df_pois['description'].notna()].sample(min(3, len(df_pois))).index\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        row = df_pois.loc[idx]\n",
    "        nom = row.get('rdfs:label', 'N/A')\n",
    "        if isinstance(nom, dict):\n",
    "            nom = nom.get('fr', nom.get('@fr', 'N/A'))\n",
    "        desc = row['description']\n",
    "        length = len(desc) if desc else 0\n",
    "        \n",
    "        print(f\"\\n{i}. {nom}\")\n",
    "        print(f\"   Type: {row.get('type_principal', 'N/A')}\")\n",
    "        print(f\"   Longueur: {length} caract√®res\")\n",
    "        print(f\"   Description: {desc[:200]}...\" if length > 200 else f\"   Description: {desc}\")\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Features pour le Quality Scorer\n",
    "\n",
    "Identifier les features importantes pour scorer la qualit√© des POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer features de compl√©tude\n",
    "print(\"‚öôÔ∏è  Calcul des features de qualit√©...\\n\")\n",
    "\n",
    "features_quality = {}\n",
    "\n",
    "# 1. Has name/label\n",
    "if 'rdfs:label' in df_pois.columns:\n",
    "    features_quality['has_name'] = df_pois['rdfs:label'].notna().astype(int)\n",
    "    print(f\"‚úÖ has_name: {features_quality['has_name'].sum():,} / {len(df_pois):,}\")\n",
    "\n",
    "# 2. Has description\n",
    "if 'description' in df_pois.columns:\n",
    "    features_quality['has_description'] = df_pois['description'].notna().astype(int)\n",
    "    features_quality['description_length'] = df_pois['description_length']\n",
    "    print(f\"‚úÖ has_description: {features_quality['has_description'].sum():,} / {len(df_pois):,}\")\n",
    "    print(f\"‚úÖ description_length: moyenne {features_quality['description_length'].mean():.1f} caract√®res\")\n",
    "\n",
    "# 3. Has GPS\n",
    "if 'latitude' in df_pois.columns:\n",
    "    features_quality['has_gps'] = df_pois['latitude'].notna().astype(int)\n",
    "    print(f\"‚úÖ has_gps: {features_quality['has_gps'].sum():,} / {len(df_pois):,}\")\n",
    "\n",
    "# 4. Has type\n",
    "if 'type_principal' in df_pois.columns:\n",
    "    features_quality['has_type'] = df_pois['type_principal'].notna().astype(int)\n",
    "    print(f\"‚úÖ has_type: {features_quality['has_type'].sum():,} / {len(df_pois):,}\")\n",
    "\n",
    "# 5. Has contact\n",
    "if 'hasContact' in df_pois.columns:\n",
    "    features_quality['has_contact'] = df_pois['hasContact'].notna().astype(int)\n",
    "    print(f\"‚úÖ has_contact: {features_quality['has_contact'].sum():,} / {len(df_pois):,}\")\n",
    "\n",
    "print(f\"\\nüìä Total features calcul√©es: {len(features_quality)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score de compl√©tude simple (baseline)\n",
    "df_features = pd.DataFrame(features_quality)\n",
    "\n",
    "# Score simplifi√© (0-100)\n",
    "df_features['completeness_score_simple'] = (\n",
    "    df_features['has_name'] * 20 +\n",
    "    df_features['has_description'] * 30 +\n",
    "    df_features['has_gps'] * 20 +\n",
    "    df_features['has_type'] * 15 +\n",
    "    df_features['has_contact'] * 15\n",
    ")\n",
    "\n",
    "print(\"üéØ Score de compl√©tude (baseline):\")\n",
    "print(f\"  Moyenne: {df_features['completeness_score_simple'].mean():.1f} / 100\")\n",
    "print(f\"  M√©diane: {df_features['completeness_score_simple'].median():.1f} / 100\")\n",
    "print(f\"  √âcart-type: {df_features['completeness_score_simple'].std():.1f}\")\n",
    "print(f\"\\nüìä Distribution:\")\n",
    "print(df_features['completeness_score_simple'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser distribution score\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme\n",
    "axes[0].hist(df_features['completeness_score_simple'], bins=20, \n",
    "             color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Score de Compl√©tude (0-100)', fontsize=11)\n",
    "axes[0].set_ylabel('Nombre de POIs', fontsize=11)\n",
    "axes[0].set_title('Distribution Score de Compl√©tude (Baseline)', fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(df_features['completeness_score_simple'].mean(), \n",
    "                color='red', linestyle='--', label=f\"Moyenne: {df_features['completeness_score_simple'].mean():.1f}\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Boxplot par qualit√©\n",
    "score_categories = pd.cut(df_features['completeness_score_simple'], \n",
    "                          bins=[0, 40, 60, 80, 100], \n",
    "                          labels=['Low (<40)', 'Medium (40-60)', 'Good (60-80)', 'Excellent (80-100)'])\n",
    "\n",
    "category_counts = score_categories.value_counts().sort_index()\n",
    "category_counts.plot(kind='bar', ax=axes[1], color=['red', 'orange', 'lightgreen', 'green'], alpha=0.7)\n",
    "axes[1].set_xlabel('Cat√©gorie de Qualit√©', fontsize=11)\n",
    "axes[1].set_ylabel('Nombre de POIs', fontsize=11)\n",
    "axes[1].set_title('R√©partition par Cat√©gorie de Qualit√©', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Ajouter valeurs\n",
    "for i, v in enumerate(category_counts.values):\n",
    "    axes[1].text(i, v + 100, f'{v:,}\\n({v/len(df_features)*100:.1f}%)', \n",
    "                 ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse D√©mographique (Communes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview communes\n",
    "print(\"üèòÔ∏è  Donn√©es Communes (Opendatasoft):\\n\")\n",
    "print(f\"  Total communes: {len(df_communes):,}\")\n",
    "print(f\"  Population totale: {df_communes['population'].sum():,.0f}\")\n",
    "print(f\"  Population moyenne: {df_communes['population'].mean():,.0f}\")\n",
    "print(f\"\\nüìä Top 10 communes par population:\")\n",
    "print(df_communes.nlargest(10, 'population')[['nom_commune', 'population', 'densite_hab_km2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution population\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme (√©chelle log)\n",
    "axes[0].hist(df_communes['population'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Population', fontsize=11)\n",
    "axes[0].set_ylabel('Nombre de communes', fontsize=11)\n",
    "axes[0].set_title('Distribution Population Communes', fontsize=12, fontweight='bold')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Densit√©\n",
    "axes[1].hist(df_communes['densite_hab_km2'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Densit√© (hab/km¬≤)', fontsize=11)\n",
    "axes[1].set_ylabel('Nombre de communes', fontsize=11)\n",
    "axes[1].set_title('Distribution Densit√© Communes', fontsize=12, fontweight='bold')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse Climat (R√©gions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview climat\n",
    "print(\"üå§Ô∏è  Donn√©es Climatiques (Open-Meteo):\\n\")\n",
    "print(df_climate[['region', 'temp_avg_annual', 'precipitation_annual_mm', 'climate_type']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation climat\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Temp√©rature moyenne par r√©gion\n",
    "df_climate_sorted = df_climate.sort_values('temp_avg_annual')\n",
    "axes[0, 0].barh(df_climate_sorted['region'], df_climate_sorted['temp_avg_annual'], \n",
    "                color='orangered', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Temp√©rature Moyenne Annuelle (¬∞C)', fontsize=10)\n",
    "axes[0, 0].set_title('Temp√©rature par R√©gion', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Pr√©cipitations par r√©gion\n",
    "df_climate_sorted = df_climate.sort_values('precipitation_annual_mm')\n",
    "axes[0, 1].barh(df_climate_sorted['region'], df_climate_sorted['precipitation_annual_mm'], \n",
    "                color='steelblue', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Pr√©cipitations Annuelles (mm)', fontsize=10)\n",
    "axes[0, 1].set_title('Pr√©cipitations par R√©gion', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Distribution types climatiques\n",
    "climate_counts = df_climate['climate_type'].value_counts()\n",
    "climate_counts.plot(kind='bar', ax=axes[1, 0], color='seagreen', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Type Climatique', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Nombre de R√©gions', fontsize=10)\n",
    "axes[1, 0].set_title('Distribution Types Climatiques', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Scatter temp vs pr√©cipitations\n",
    "colors_map = {'mediterranean': 'orangered', 'oceanic': 'steelblue', \n",
    "              'continental': 'gold', 'mountain': 'purple'}\n",
    "for climate_type in df_climate['climate_type'].unique():\n",
    "    data = df_climate[df_climate['climate_type'] == climate_type]\n",
    "    axes[1, 1].scatter(data['temp_avg_annual'], data['precipitation_annual_mm'], \n",
    "                       label=climate_type, s=100, alpha=0.7, \n",
    "                       color=colors_map.get(climate_type, 'gray'))\n",
    "\n",
    "axes[1, 1].set_xlabel('Temp√©rature Moyenne (¬∞C)', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Pr√©cipitations (mm)', fontsize=10)\n",
    "axes[1, 1].set_title('Temp√©rature vs Pr√©cipitations par Type', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions & Next Steps\n",
    "\n",
    "### üìä R√©sum√© des Donn√©es\n",
    "\n",
    "**POIs DATAtourisme:**\n",
    "- 50k POIs collect√©s\n",
    "- Compl√©tude variable selon les champs (√† am√©liorer pour le scoring)\n",
    "- Distribution g√©ographique couvre toute la France\n",
    "- Types vari√©s (h√©bergement, restauration, culture, activit√©s)\n",
    "\n",
    "**Communes & Climat:**\n",
    "- 10k communes avec donn√©es d√©mographiques\n",
    "- 13 r√©gions avec donn√©es climatiques\n",
    "- Pr√™t pour enrichissement g√©ographique\n",
    "\n",
    "### üéØ Features Identifi√©es pour Quality Scorer\n",
    "\n",
    "**Compl√©tude (40%):**\n",
    "- has_name, has_description, has_gps, has_type, has_contact\n",
    "- description_length\n",
    "\n",
    "**Richesse Textuelle (30%):**\n",
    "- description_richness (√† calculer avec textstat)\n",
    "- nb_languages (multilinguisme)\n",
    "\n",
    "**Contexte G√©o (20%):**\n",
    "- commune_population, commune_density\n",
    "- poi_density_radius (nb POIs similaires √† proximit√©)\n",
    "- climate_type\n",
    "\n",
    "**Freshness (10%):**\n",
    "- last_update_days (si disponible)\n",
    "\n",
    "### ‚è≠Ô∏è Next Steps\n",
    "\n",
    "1. **Feature Engineering** (Notebook 02):\n",
    "   - Calculer les 20 features du Quality Scorer\n",
    "   - Enrichir avec donn√©es communes/climat\n",
    "   - Cr√©er target synth√©tique (score 0-100)\n",
    "\n",
    "2. **Quality Scorer Training** (Notebook 03):\n",
    "   - Entra√Æner LightGBM/XGBoost\n",
    "   - Tuning hyperparam√®tres\n",
    "   - Validation (target R¬≤ > 0.75)\n",
    "\n",
    "3. **Gap Detection** (Notebook 04):\n",
    "   - HDBSCAN clustering\n",
    "   - Analyse distribution types\n",
    "   - Random Forest opportunit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder version enrichie pour le prochain notebook\n",
    "print(\"üíæ Sauvegarde des donn√©es enrichies...\")\n",
    "\n",
    "# Combiner POIs avec features de base\n",
    "df_pois_enriched = df_pois.copy()\n",
    "for col in df_features.columns:\n",
    "    df_pois_enriched[col] = df_features[col]\n",
    "\n",
    "output_file = Path(\"../data/processed/pois_enriched_eda.parquet\")\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_pois_enriched.to_parquet(output_file, index=False, compression='snappy')\n",
    "\n",
    "print(f\"‚úÖ Sauvegard√©: {output_file}\")\n",
    "print(f\"   Records: {len(df_pois_enriched):,}\")\n",
    "print(f\"   Colonnes: {len(df_pois_enriched.columns)}\")\n",
    "print(f\"   Taille: {output_file.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NOTEBOOK 1 - EDA TERMIN√â\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìà Prochaine √©tape: Notebook 02 - Feature Engineering pour Quality Scorer\")\n",
    "print(\"\\nüéØ Objectif: Cr√©er les 20 features ML et le target score synth√©tique (0-100)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
