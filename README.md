# Alpine Guide ML

**SystÃ¨me de machine learning pour l'Ã©valuation de la qualitÃ© des points d'intÃ©rÃªt touristiques**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-green.svg)](https://fastapi.tiangolo.com)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.4.0-orange.svg)](https://scikit-learn.org)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)

---

## Vue d'ensemble

**Alpine Guide ML** est un projet d'exploration appliquÃ©e du machine learning au domaine du tourisme. Le systÃ¨me Ã©value automatiquement la qualitÃ© de points d'intÃ©rÃªt touristiques Ã  partir de donnÃ©es publiques (DATAtourisme), permettant d'identifier et prioriser les POIs les mieux documentÃ©s.

Ce projet dÃ©montre un pipeline ML complet, de la collecte de donnÃ©es Ã  l'API de production, avec une interface conversationnelle pour illustrer une application concrÃ¨te des prÃ©dictions.

### Composants principaux

**Pipeline ML de scoring**
- Ã‰valuation automatique de 50 000+ POIs sur une Ã©chelle 0-100
- ModÃ¨le Gradient Boosting avec 17 features engineered
- Performance : RÂ² = 0.9787, MAE = 4.82 points
- API FastAPI temps rÃ©el (<8ms d'infÃ©rence)

**Interface conversationnelle**
- Widget JavaScript embeddable utilisant les scores ML
- Orchestrateur IA (Gemini 2.0 Flash) pour recommandations
- Architecture multi-tenant avec configuration par territoire
- DÃ©montre l'intÃ©gration des prÃ©dictions dans un produit utilisateur

---

## Architecture systÃ¨me

### Vue d'ensemble

```mermaid
graph TB
    subgraph sources ["Sources de donnÃ©es"]
        A1[DATAtourisme]
        A2[Open-Meteo]
        A3[Hub'Eau]
        A4[INSEE]
        A5[Opendatasoft]
    end

    subgraph ml ["Pipeline ML"]
        B1[Collecteurs]
        B2[Feature Engineering]
        B3[EntraÃ®nement GB]
        B4[ModÃ¨le scorer.pkl]
        B5[API Scoring]
    end

    subgraph app ["Application"]
        C1[Orchestrateur IA]
        C2[API Chat]
        C3[Widget JS]
    end

    subgraph infra ["Infrastructure"]
        D1[Redis Cache]
        D2[Nginx]
        D3[Prometheus]
    end

    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1
    A5 --> B1

    B1 --> B2
    B2 --> B3
    B3 --> B4
    B4 --> B5

    B5 --> C1
    C1 --> C2
    C2 --> C3

    B5 --> D1
    C2 --> D1
    D2 --> B5
    D2 --> C2
    B5 --> D3
    C2 --> D3

    style B4 fill:#90EE90
    style B5 fill:#87CEEB
    style C1 fill:#FFD700
```

### Stack technique

| Composant | Technologies | Usage |
|-----------|-------------|-------|
| **ML Backend** | Python 3.11, scikit-learn, pandas, FastAPI | Pipeline ML et scoring |
| **Chatbot Backend** | Python 3.11, Gemini 2.0 Flash, FastAPI | Orchestration conversationnelle |
| **Frontend** | JavaScript vanilla, CSS3 | Widget embeddable |
| **Cache** | Redis 7 | Performance (85% hit rate) |
| **Proxy** | Nginx | Load balancing, SSL |
| **Monitoring** | Prometheus, Grafana | MÃ©triques temps rÃ©el |
| **DÃ©ploiement** | Docker, Docker Compose | Conteneurisation |

---

## Pipeline machine learning

### ProblÃ©matique explorÃ©e

Les donnÃ©es touristiques publiques (DATAtourisme, OpenStreetMap) prÃ©sentent une qualitÃ© trÃ¨s hÃ©tÃ©rogÃ¨ne :
- 45% des POIs sans horaires d'ouverture
- 62% sans contact email
- Pas de mÃ©trique de qualitÃ© standardisÃ©e

L'objectif est d'Ã©valuer automatiquement cette qualitÃ© pour permettre un tri et une priorisation des POIs.

### Sources de donnÃ©es

Le systÃ¨me agrÃ¨ge 5 sources publiques :

| Source | Type | Volume | Usage |
|--------|------|--------|-------|
| **DATAtourisme** | POIs touristiques | 50 000+ | Base principale (nom, description, GPS, images) |
| **Open-Meteo** | MÃ©tÃ©o | 13 rÃ©gions | Contexte climatique |
| **Hub'Eau** | TempÃ©rature eau | 1 000+ sites | QualitÃ© baignade |
| **INSEE MELODI** | Socio-Ã©conomique | 10 000 communes | Contexte territorial |
| **Opendatasoft** | DÃ©mographie | Toutes communes | Population, densitÃ© |

### Feature engineering

**17 features construites en 4 catÃ©gories :**

#### 1. ComplÃ©tude (7 features binaires)
```python
features = {
    "has_name": 1.0 if poi["name"] else 0.0,
    "has_description": 1.0 if poi["description"] else 0.0,
    "has_gps": 1.0 if (lat and lon) else 0.0,
    "has_address": 1.0 if poi["address"] else 0.0,
    "has_images": 1.0 if poi["images"] else 0.0,
    "has_opening_hours": 1.0 if poi["hours"] else 0.0,
    "has_contact": 1.0 if (phone or email) else 0.0
}
```

#### 2. Richesse (3 features continues)
```python
features = {
    "description_length": len(poi["description"]),
    "num_images": len(poi["images"]),
    "has_website": 1.0 if poi["website"] else 0.0
}
```

#### 3. Contexte territorial (4 features enrichies)
```python
features = {
    "insee_salary_median": get_commune_salary(poi["gps"]),
    "population": get_commune_population(poi["gps"]),
    "poi_density_10km": count_nearby_pois(poi["gps"], 10),
    "latitude": poi["latitude"],
    "longitude": poi["longitude"]
}
```

#### 4. FraÃ®cheur (2 features temporelles)
```python
features = {
    "days_since_update": days_since_last_update(poi),
    "is_recent": 1.0 if days_since_update <= 180 else 0.0
}
```

### SÃ©lection du modÃ¨le

**Comparaison de 4 algorithmes :**

| Algorithme | RÂ² Test | MAE Test | InfÃ©rence |
|------------|---------|----------|-----------|
| **Gradient Boosting** | **0.9787** | **4.82** | 4-8ms |
| Random Forest | 0.9521 | 6.15 | 6-10ms |
| XGBoost | 0.9695 | 5.20 | 5-9ms |
| RÃ©gression linÃ©aire | 0.7542 | 12.80 | 1ms |

**Choix : Gradient Boosting Regressor (scikit-learn)**
- Meilleure performance sur le dataset
- InfÃ©rence rapide sur CPU
- InterprÃ©tabilitÃ© via feature importance
- Pas besoin de GPU

### RÃ©sultats du modÃ¨le

**MÃ©triques (test set) :**
- RÂ² = 0.9787 (97.87% de variance expliquÃ©e)
- MAE = 4.82 points (sur Ã©chelle 0-100)
- RMSE = 6.93

**Distribution des erreurs :**
- 72.4% : erreur <5 points
- 21.0% : erreur 5-10 points
- 4.9% : erreur 10-15 points
- 1.7% : erreur >15 points

**Importance des features (Top 5) :**
```
description_length    32.5%  (qualitÃ© descriptive)
has_description       18.2%  (prÃ©sence description)
num_images            14.6%  (richesse visuelle)
poi_density_10km       9.9%  (contexte touristique)
insee_salary_median    7.7%  (contexte socio-Ã©conomique)
```

### EntraÃ®nement

```bash
# Script complet
cd backend/ml/training
python 03_train_quality_scorer.py

# GÃ©nÃ¨re :
# - backend/ml/models/quality_scorer/scorer.pkl
# - backend/ml/models/quality_scorer/metrics.json
# - backend/ml/models/quality_scorer/features.txt
```

### API de scoring

```python
# backend/api/main.py
@app.post("/score-poi")
async def score_poi(poi_data: POIScoreRequest):
    result = scorer.score_poi(poi_data.dict())
    return {
        "quality_score": result.quality_score,
        "confidence": result.confidence,
        "model_version": result.model_version
    }
```

**Performance :**
- P50 latency : 6ms
- P95 latency : 48ms (cold) / 3ms (cached)
- Cache hit rate : 85%+
- Throughput : 180 req/s (4 workers)

---

## Interface conversationnelle

### RÃ´le dans le systÃ¨me

Le widget dÃ©montre l'utilisation concrÃ¨te des scores ML dans une application utilisateur. Il intÃ¨gre un orchestrateur IA (Gemini 2.0 Flash) qui utilise les prÃ©dictions pour prioriser les recommandations touristiques.

**Pourquoi ce composant dans un projet ML ?**

Dans un projet ML rÃ©el, le modÃ¨le n'est qu'une brique technique. Le widget montre comment exploiter les prÃ©dictions dans un produit utilisateur complet, avec gestion de contexte, enrichissement multi-sources et interface naturelle.

### Architecture dÃ©taillÃ©e

Le systÃ¨me repose sur 3 couches interdÃ©pendantes :

#### 1. Frontend JavaScript (widget/)

**Composant embeddable autonome** :
```javascript
// alpine-guide-widget.js (800+ lignes)
class AlpineGuideWidget {
    constructor(config) {
        this.config = {
            territory: 'annecy',      // Configuration multi-tenant
            apiBase: 'https://...',   // Backend conversationnel
            primaryColor: '#0066CC',   // Personnalisation visuelle
            persistHistory: true       // Persistance localStorage
        };
        this.state = {
            sessionId: generateSessionId(),
            conversations: [],
            isTyping: false
        };
    }

    async init() {
        await this.loadTerritoryConfig();  // Charge config YAML
        this.createWidget();                // Injecte DOM + CSS
        this.attachEvents();                // Listeners user input
    }
}
```

**Cycle de vie du widget** :
1. **Chargement** : Script injectÃ© dans page hÃ´te (`<script src="...">`)
2. **Initialisation** : RÃ©cupÃ©ration config territoire depuis backend
3. **Rendu** : Injection HTML/CSS dans shadow DOM (isolation styles)
4. **Connexion** : WebSocket ou polling vers API chatbot (:8001)
5. **Interaction** : Capture input â†’ envoi backend â†’ affichage rÃ©ponse
6. **Persistance** : Sauvegarde historique dans localStorage

**FonctionnalitÃ©s clÃ©s** :
- Auto-complÃ©tion et suggestions contextuelles
- Indicateur de frappe temps rÃ©el
- Gestion offline (cache local)
- ThÃ¨mes clair/sombre automatiques
- Responsive (mobile + desktop)
- A11y (navigation clavier, ARIA labels)

#### 2. Orchestrateur IA (backend/core/)

**Cerveau du systÃ¨me conversationnel** :

```python
# orchestrator.py
class YAMLOrchestrator:
    def __init__(self, yaml_path, gemini_api_key, rag_service,
                 weather_service, supabase_service):
        self.intents = self._load_intents_from_yaml(yaml_path)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        self.rag_service = rag_service
        # ... autres services

    async def process_message(self, user_message, conversation_state):
        # 1. DÃ©tection d'intent via Gemini
        intent = await self._detect_intent(user_message)

        # 2. Extraction des slots (entitÃ©s)
        slots = await self._extract_slots(user_message, intent)

        # 3. Appel services externes selon intent
        if intent == 'restaurant':
            pois = await self.rag_service.search_pois(
                type='restaurant',
                location=slots['localisation']
            )
            # Filtrage par score ML
            pois = [p for p in pois if p.quality_score >= 70]

        # 4. GÃ©nÃ©ration rÃ©ponse enrichie
        response = await self._generate_response(intent, slots, pois)
        return response
```

**Pipeline de traitement NLU** :

| Ã‰tape | Technique | Exemple |
|-------|-----------|---------|
| **Normalisation** | Lowercase, accents | "OÃ¹ MANGER ?" â†’ "ou manger" |
| **DÃ©tection intent** | Gemini 2.0 Flash | "restaurant italien" â†’ `intent: restaurant` |
| **Extraction slots** | NER + patterns | "demain Ã  Annecy" â†’ `{date: 2025-11-13, localisation: Annecy}` |
| **Validation** | Contraintes YAML | Slots obligatoires prÃ©sents ? |
| **Clarification** | Templates YAML | Manque slot â†’ "Dans quel secteur ?" |
| **Enrichissement** | APIs externes | Ajout mÃ©tÃ©o, tempÃ©rature eau |
| **GÃ©nÃ©ration** | Gemini contextuel | RÃ©ponse naturelle structurÃ©e |

**Configuration des intents (intents_slots.yaml - 386 lignes)** :

```yaml
intents:
  restaurant:
    description: "Recherche de restaurants"
    slots_obligatoires: []
    slots_optionnels:
      - localisation
      - type_cuisine
      - budget
      - regime_alimentaire
    exemple_demande_clarification:
      - utilisateur: "Je cherche un restaurant"
        clarification: "Dans quel secteur et pour quelle date ?"

  water_temperature:
    description: "TempÃ©rature de l'eau des lacs"
    slots_obligatoires:
      - plan_eau
    slots_optionnels:
      - localisation
      - date
```

**17 intents implÃ©mentÃ©s** : mÃ©tÃ©o, restaurant, randonnÃ©e, ski, baignade, Ã©vÃ©nements, musÃ©es, transports, urgences, etc.

#### 3. Configuration multi-tenant (backend/config/territories/)

**Un fichier YAML par territoire** (annecy.yaml, chambery.yaml, chamonix.yaml) :

```yaml
# annecy.yaml (597 lignes)
territory:
  slug: annecy
  name: "Annecy - Lac et Montagnes"

  # Branding (identitÃ© visuelle complÃ¨te)
  branding:
    appName: "Explore Annecy"
    colors:
      primary: "#0066CC"    # Bleu lac
      accent: "#FF6B35"     # Orange montagne
    assets:
      logo: "https://cdn.../logo-annecy.svg"
      chatAvatar: "https://cdn.../avatar-guide.png"

  # GÃ©ographie (coordonnÃ©es, limites, landmarks)
  geography:
    center: {lat: 45.8992, lng: 6.1294}
    bounds:
      north: 46.0500
      south: 45.7500
    landmarks:
      - name: "Lac d'Annecy"
        coordinates: [45.8631, 6.1639]
        type: "natural"

  # PersonnalitÃ© IA
  ai:
    personality:
      tone: "chaleureux et expert local"
      style: "conversationnel et informatif"
    specialties:
      - name: "Lac d'Annecy"
        keywords: ["lac", "baignade", "pÃ©dalo"]
      - name: "Gastronomie savoyarde"
        keywords: ["reblochon", "tartiflette"]

  # Plans d'eau avec tempÃ©ratures saisonniÃ¨res
  waterBodies:
    primary:
      name: "Lac d'Annecy"
      temperatures:
        ete: {min: 18, max: 24, typical: 21}
        hiver: {min: 4, max: 7, typical: 5}

  # Suggestions intelligentes par contexte
  smartSuggestions:
    byIntent:
      restaurant:
        - "Restaurant vue lac ?"
        - "SpÃ©cialitÃ©s savoyardes ?"
```

**Isolation multi-tenant** :
- Chaque territoire = config indÃ©pendante
- DonnÃ©es POIs filtrÃ©es par gÃ©ographie
- Branding personnalisÃ© (couleurs, logo, messages)
- Intents activÃ©s/dÃ©sactivÃ©s par territoire
- Quotas et analytics sÃ©parÃ©s

### IntÃ©gration ML â†’ Widget

**Flux complet d'une requÃªte utilisateur** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Utilisateur â”‚ "Quels restaurants Ã  Annecy ?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Widget JavaScript  â”‚ Envoi POST /chat
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestrateur (core) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. DÃ©tection intent  â”‚ â†’ Gemini 2.0 Flash : "restaurant"
â”‚ 2. Extraction slots  â”‚ â†’ {localisation: "Annecy"}
â”‚ 3. Appel RAG         â”‚ â†’ Supabase : rÃ©cup 50 restaurants Annecy
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API ML Scoring     â”‚ POST /score-batch
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: 50 POIs     â”‚
â”‚ Output: scores     â”‚ [POI1: 85/100, POI2: 72/100, POI3: 45/100, ...]
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Filtrage ML (core)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Garde score >= 70  â”‚ â†’ 18 restaurants conservÃ©s
â”‚ â€¢ Tri dÃ©croissant    â”‚ â†’ [POI1: 85, POI2: 72, ...]
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enrichissement       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ MÃ©tÃ©o API          â”‚ â†’ "Temps ensoleillÃ© 22Â°C"
â”‚ â€¢ TempÃ©rature eau    â”‚ â†’ "Lac d'Annecy : 21Â°C"
â”‚ â€¢ Distance GPS       â”‚ â†’ Calcul si user_location disponible
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GÃ©nÃ©ration (Gemini)  â”‚ Prompt enrichi avec POIs + contexte
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "Voici 3 excellents  â”‚
â”‚ restaurants avec une â”‚
â”‚ belle vue sur le lac â”‚
â”‚ (parfait avec ce     â”‚
â”‚ temps ensoleillÃ©!) : â”‚
â”‚                      â”‚
â”‚ 1. La Voile (85/100) â”‚
â”‚    - Vue panoramique â”‚
â”‚    - SpÃ©cialitÃ©s lac â”‚
â”‚ ..."                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Widget JavaScript  â”‚ Affichage rÃ©ponse formatÃ©e
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ParamÃ¨tres d'intÃ©gration ML configurables** :

```yaml
# Dans chaque fichier territoire
ml_integration:
  scoring_api: "http://ml-api:8000"
  min_quality_score: 70           # Seuil de filtrage
  boost_high_quality: true        # Prioriser scores >85
  max_results: 5                  # Top-N aprÃ¨s tri
  score_weight: 0.4               # PondÃ©ration ML vs distance
  explain_scores: false           # Afficher scores Ã  user ?
```

### Exemples de conversations rÃ©elles

**Exemple 1 : Recherche restaurant**

```
User: "Un bon restaurant italien pas trop cher Ã  Annecy"

Widget â†’ Backend:
{
  "message": "Un bon restaurant italien pas trop cher Ã  Annecy",
  "session_id": "sess_abc123",
  "territory": "annecy"
}

Backend processing:
- Intent: restaurant
- Slots: {cuisine: "italien", budget: "moyen", localisation: "Annecy"}
- RAG: 12 restaurants italiens trouvÃ©s
- ML scoring: [8 avec score >= 70]
- Filtrage budget: 5 conservÃ©s
- Tri: [La Trattoria: 82, Casa Mia: 78, ...]

Bot: "Voici 3 trattorias authentiques Ã  prix doux :

1. **La Trattoria du Lac** (82/100)
   ğŸ“ 12 rue PerriÃ¨re, Vieille Ville
   ğŸ’° Menu 18-25â‚¬
   â­ PÃ¢tes fraÃ®ches maison, terrasse sur canal

2. **Casa Mia** (78/100)
   ğŸ“ 5 rue Royale
   ğŸ’° Pizzas 12-16â‚¬
   â­ Four Ã  bois, ingrÃ©dients bio

MÃ©tÃ©o actuelle : â˜€ï¸ 22Â°C, parfait pour une terrasse !"
```

**Exemple 2 : TempÃ©rature eau + conseil**

```
User: "Il fait chaud l'eau du lac ?"

Backend:
- Intent: water_temperature
- Slot: plan_eau â†’ "lac d'Annecy" (infÃ©rÃ© du territoire)
- Service Hub'Eau: 21.2Â°C (mesure temps rÃ©el)
- Contexte saisonnier (Ã©tÃ©): "tempÃ©rature optimale"

Bot: "ğŸŒ¡ï¸ TempÃ©rature du lac d'Annecy : **21Â°C** (mesure actuelle)

C'est parfait pour la baignade ! ğŸŠâ€â™‚ï¸

Plages surveillÃ©es recommandÃ©es :
â€¢ Plage des Marquisats (10 min centre-ville)
â€¢ Plage d'Albigny (baie protÃ©gÃ©e, +1Â°C)

Conseil : l'eau est encore plus chaude en surface l'aprÃ¨s-midi â˜€ï¸"
```

**Exemple 3 : Clarification multi-tour**

```
User: "Je veux faire une randonnÃ©e"

Bot: "Excellente idÃ©e ! Pour te conseiller au mieux :
- Dans quel secteur ? (Semnoz, Parmelan, Tournette...)
- Quelle durÃ©e ? (1h, demi-journÃ©e, journÃ©e)
- Quel niveau ? (facile, moyen, difficile)"

User: "Facile 2h max vers le Semnoz"

Backend:
- Intent: randonnee
- Slots complÃ©tÃ©s: {localisation: "Semnoz", durÃ©e: "2h", difficultÃ©: "facile"}
- RAG: 8 randonnÃ©es trouvÃ©es
- ML: filtrage scores >= 70 â†’ 5 conservÃ©es
- MÃ©tÃ©o: vÃ©rification conditions

Bot: "Top 2 balades faciles au Semnoz (2h) :

1. **Sentier des CrÃªtes** (Score: 88/100)
   ğŸ“ DÃ©part parking tÃ©lÃ©phÃ©rique
   â±ï¸ 1h45 boucle
   ğŸ“ˆ +150m dÃ©nivelÃ©
   ğŸŒ„ Vue panoramique lac + Mont Blanc

MÃ©tÃ©o : â˜€ï¸ Parfait aujourd'hui (22Â°C vallÃ©e, 18Â°C sommet)
Conseil : Chaussures de marche suffisantes !"
```

### Configuration et dÃ©ploiement

**IntÃ©gration dans un site (1 ligne)** :

```html
<!-- MÃ©thode 1 : Script tag simple -->
<script src="https://cdn.alpine-guide.com/widget.js"
        data-territory="annecy"
        data-api-key="YOUR_API_KEY"></script>

<!-- MÃ©thode 2 : Configuration avancÃ©e -->
<script>
  window.AlpineGuideConfig = {
    territory: 'annecy',
    apiKey: 'YOUR_API_KEY',
    theme: 'auto',           // light, dark, auto
    position: 'bottom-right',
    autoOpen: false,
    language: 'fr',
    primaryColor: '#0066CC',
    onReady: (widget) => {
      console.log('Widget prÃªt');
    }
  };
</script>
<script src="https://cdn.alpine-guide.com/widget.js"></script>
```

**Options de personnalisation disponibles** :

| Option | Type | Description |
|--------|------|-------------|
| `territory` | string | Territoire (annecy, chambery, chamonix) |
| `theme` | string | ThÃ¨me visuel (light, dark, auto) |
| `position` | string | Position (bottom-right, bottom-left, top-right) |
| `primaryColor` | string | Couleur principale (#hex) |
| `language` | string | Langue (fr, en, de, it, es) |
| `autoOpen` | boolean | Ouverture auto aprÃ¨s 5s |
| `persistHistory` | boolean | Sauvegarde historique local |
| `welcomeMessage` | string | Message d'accueil personnalisÃ© |

---

## DÃ©marrage rapide

### PrÃ©requis

- Docker & Docker Compose
- Python 3.11+ (dÃ©veloppement local)
- 4GB RAM minimum

### Lancement complet

```bash
# 1. Cloner
git clone https://github.com/Aguern/alpine-guide-ML.git
cd alpine-guide-ML

# 2. Configuration
cp .env.example .env
# Ã‰diter .env si besoin (defaults OK en local)

# 3. Lancer tous les services
docker-compose -f docker-compose.full-stack.yml up -d
```

**Services disponibles :**

| Service | URL | Description |
|---------|-----|-------------|
| ML API | http://localhost:8000/docs | API scoring (OpenAPI) |
| Chatbot API | http://localhost:8001/docs | API conversationnelle |
| Widget | http://localhost/widget | Widget embeddable |
| Admin | http://localhost/admin | Config widget |
| Grafana | http://localhost:3000 | Monitoring |

### Test API

```bash
curl -X POST "http://localhost:8000/score-poi" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Mont Blanc",
    "description": "Plus haut sommet des Alpes",
    "latitude": 45.8326,
    "longitude": 6.8652,
    "num_images": 15
  }'
```

---

## Structure du projet

```
alpine-guide-ML/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ml/                        # Pipeline ML
â”‚   â”‚   â”œâ”€â”€ training/              # Scripts entraÃ®nement
â”‚   â”‚   â”œâ”€â”€ inference/             # Scoring production
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ quality_scorer/
â”‚   â”‚           â”œâ”€â”€ scorer.pkl
â”‚   â”‚           â”œâ”€â”€ metrics.json
â”‚   â”‚           â””â”€â”€ features.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                      # Data engineering
â”‚   â”‚   â”œâ”€â”€ ingestion/             # Collecteurs APIs
â”‚   â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â”‚   â””â”€â”€ processed/             # Features ML
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                       # APIs FastAPI
â”‚   â”‚   â”œâ”€â”€ main.py                # ML scoring :8000
â”‚   â”‚   â””â”€â”€ chat_endpoint.py       # Chatbot :8001
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                      # Orchestrateur IA
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ intents_slots.yaml
â”‚   â”‚   â””â”€â”€ cache_manager.py
â”‚   â”‚
â”‚   â””â”€â”€ config/                    # Config territoires
â”‚
â”œâ”€â”€ widget/                         # Frontend
â”‚   â”œâ”€â”€ alpine-guide-widget.js
â”‚   â”œâ”€â”€ styles.css
â”‚   â””â”€â”€ admin-simple/
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ docker/
â”‚   â””â”€â”€ monitoring/
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”‚
â”œâ”€â”€ docker-compose.full-stack.yml
â””â”€â”€ README.md
```

---

## Tests

**45+ tests automatisÃ©s (pytest)**

### Tests unitaires

```python
# tests/unit/test_poi_scorer.py
def test_extract_features_complete_poi():
    scorer = POIQualityScorer()
    features = scorer.extract_features(complete_poi)

    assert len(features) == 17
    assert features["has_name"] == 1.0
    assert features["num_images"] == 25.0

def test_score_poi_returns_valid_result():
    result = scorer.score_poi(sample_poi)

    assert 0 <= result.quality_score <= 100
    assert 0 <= result.confidence <= 1.0
```

### Tests d'intÃ©gration

```python
# tests/integration/test_api.py
def test_api_score_poi_endpoint(client):
    response = client.post("/score-poi", json=poi_data)

    assert response.status_code == 200
    assert "quality_score" in response.json()
```

```bash
# ExÃ©cution
pytest -v
pytest --cov=backend --cov-report=html
```

---

## Performances

### MÃ©triques ML

| MÃ©trique | Valeur |
|----------|--------|
| RÂ² Score | 0.9787 |
| MAE | 4.82/100 |
| Temps infÃ©rence | 4-8ms |
| Dataset | 50 000+ POIs |

### SystÃ¨me

| Composant | MÃ©trique | Valeur |
|-----------|----------|--------|
| ML API | P95 latency | 48ms (cold) / 3ms (cached) |
| Chatbot API | P95 latency | 150ms |
| Cache | Hit rate | 85%+ |
| Throughput | Req/s | 180 (4 workers) |

### ScalabilitÃ©

- 100 clients concurrents : P99 <145ms
- 1000 req/min : 0% erreur
- Scaling horizontal : testÃ© 4 instances

---

## Endpoints API

### ML API (port 8000)

**POST /score-poi**
```json
Request:
{
  "name": "string",
  "latitude": float,
  "longitude": float,
  "description": "string (optional)",
  "num_images": int
}

Response:
{
  "quality_score": 78.5,
  "confidence": 0.87,
  "model_version": "20251112_120000"
}
```

**POST /score-batch**
Score multiple POIs en une requÃªte.

**GET /model/info**
MÃ©tadonnÃ©es et performance du modÃ¨le.

---

## DÃ©ploiement

### Docker Compose

```bash
# Build
docker-compose -f docker-compose.full-stack.yml build

# Lancement
docker-compose -f docker-compose.full-stack.yml up -d

# Logs
docker-compose logs -f ml-api

# ArrÃªt
docker-compose down
```

### Services dÃ©ployÃ©s

- `ml-api` : API scoring ML (:8000)
- `chatbot-api` : API conversationnelle (:8001)
- `redis` : Cache (:6379)
- `web` : Nginx + Widget (:80)
- `prometheus` : MÃ©triques (:9090)
- `grafana` : Dashboards (:3000)

---

## Bonnes pratiques implÃ©mentÃ©es

### MLOps
- Versioning modÃ¨le (scorer.pkl + metrics.json)
- Features reproductibles (scripts ingestion)
- API documentÃ©e (OpenAPI)
- Tests automatisÃ©s
- Monitoring (Prometheus)
- Logging structurÃ©

### DevOps
- Conteneurisation Docker
- Orchestration docker-compose
- Health checks
- Cache intelligent
- Reverse proxy
- SSL/TLS ready

### Software Engineering
- Type hints (Pydantic, typing)
- Design patterns
- Tests unitaires + intÃ©gration
- Documentation
- Error handling

---

## Licence

**Copyright (c) 2025 Nicolas Angougeard. Tous droits rÃ©servÃ©s.**

Ce projet est un portfolio technique personnel. Le code source est fourni Ã  titre de dÃ©monstration uniquement et n'est pas destinÃ© Ã  une utilisation commerciale par des tiers sans autorisation expresse.
